# 中文特定领域医疗 RAG 问答系统

本项目构建了一个基于Qwen-2.5-7B大模型与CMedQA2数据集的医疗领域RAG（检索增强生成）问答系统，旨在通过本地知识库检索为用户提供严谨、准确的医疗咨询与建议。
- **github仓库**：https://github.com/Golden740/SX2508014---01-NLP 。
## 目前已实现内容
- **环境部署**：在AutoDL平台上完成RTX 5090单卡环境下的模型加载与向量数据库配置。
- **知识库构建**： 基于CMedQA2医疗数据集，使用bge-small-zh-v1.5嵌入模型构建Chroma向量数据库。
- **系统迭代**: 经历了从基础脚本运行到Gradio网页端界面的多次迭代，解决了 Gradio5.x数据格式兼容性、frpc内网穿透下载失败以及模型回复冗余重复等关键问题。
- **UI定制**: 实现了一套双栏交互界面，左侧进行AI流式对话，右侧实时展示知识库检索溯源结果。
  
## 运行环境

- **平台** ： AutoDL (https://www.autodl.com/)。
- **镜像配置**： PyTorch==2.1.0 Python==3.10(ubuntu22.04) CUDA==12.1。
- **GPU** ： RTX 5090(32GB)。
- **CPU** ： 25 vCPU Intel(R) Xeon(R) Platinum 8470Q。
- **关键库** ： gradio, transformers, langchain, langchain-community, chromadb, modelscope。

## 依赖安装

**执行命令**: pip install gradio transformers langchain langchain-huggingface langchain-community chromadb modelscope


## 数据与模型

- **基础模型** : Qwen-2.5-7B-Instruct
- **Embedding模型** ： BAAI/bge-small-zh-v1.5
- **向量数据库**：Chroma DB
- **数据集**：CMedQA2（中文医疗问答数据集）

## 文件作用介绍

使用 Qwen-2.5-7B-instruct 为基础模型
在AutoDL中，在 /root/autodl-tmp 路径下新建 model_download.py 用于下载完整的模型。
``` Python
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer
import os
model_dir = snapshot_download('qwen/Qwen2.5-7B-Instruct', cache_dir='/root/autodl-tmp', revision='master')
```


## RAG数据流向说明
- **数据准备**: process_csv.py(清洗CSV) -> build_db.py (向量化并存入Chroma)。
- **检索阶段**: app_gradio.py接收输入 -> 调用retriever在chroma_db中寻找最相似的3条医疗记录。
- **生成阶段**: 将检索结果作为上下文（Context）输入给Qwen-2.5模型 -> 输出精简建议。


## 文件作用介绍
- **1.process_csv.py**: 专门用于处理原始的CMedQA2数据集。它负责读取原始的CSV问答对文件，进行去重、异常值过滤，并按照医疗问答的逻辑重新格式化数据，为后续的向量化做准备。
- **2.build_db.py**: 该脚本调用langchain-text-splitters对清洗后的医疗数据进行分块，随后使用bge-small-zh-v1.5嵌入模型将文本转化为高维向量，并持久化存储到本地的chroma_db文件夹中。
- **3.main_rag.py**: RAG系统的核心推理逻辑脚本，用于测试本地检索与生成流程。
- **4.app_gradio.py**: 基于Gradio构建的Web端问答界面，支持流式输出与检索溯源展示。
- **5.evaluate.py**: 该脚本脚本集成了“检索 -> Prompt 构造 -> 模型生成 -> 结果清理 -> 指标计算”的完整链路，用于在推理阶段对模型进行客观的定量评价。
- **6.prepare_sft_data_pro.py**: 本项目微调阶段的核心，专门用于生产具备“抗干扰”能力的强化版SFT（指令微调）数据集，自动将生成的样本按 9:1 的比例切分为训练集和测试集。

## 启动方式

streamlit run main_rag.py


## 系统迭代历程

### 迭代 1：基础功能实现与报错调试

- 实现了基础的检索问答逻辑。
- **解决问题**: 修复了`VectorStoreRetriever`缺少`get_relevant_documents`的`AttributeError`，统一升级为LangChain 1.x的`invoke`接口。

### 迭代 2：Gradio 界面美化与格式兼容

- 引入`gr.Blocks`实现双栏布局。
- **解决问题**: 针对Gradio不同版本的报错，将对话历史从列表格式重构为符合新版规范的字典格式。

### 迭代 3：生成质量优化

- 优化了Prompt模板，引入ChatML格式 (`<|im_start|>`)。
- **解决问题**: 通过调整`temperature=0.3`和`repetition_penalty=1.2`，解决了AI回复冗余、复读以及原样复述Prompt资料内容的问题。

### 迭代 4：回复质量优化

- 优化了Prompt模板，使用结构化指令。
- **解决问题**: 通过精细化Prompt工程，使模型具备了自动结构化输出的能力。系统能够自动识别医疗建议中的关键动作，并以Markdown列表形式呈现，极大地提升了用户阅读体验和信息的易读性。

### 迭代 5：长上下文与抗干扰强化
- 重构数据生成脚本，引入噪声样本;部署ms-swift进行LoRA微调;升级评估脚本，支持K=50压测。
- **解决问题**: 解决了长文本环境下模型抓不住重点的问题；修复了PyTorch与 transformers版本不兼容导致的_register_pytree_node报错；解决了 Gradio 界面在处理字典格式消息时的Data incompatible异常

## 运行截图（第一版，未微调）
### 1. 核心交互界面
![双栏交互界面截图](https://github.com/Golden740/SX2508014---01-NLP/blob/main/images/ui_main.png)

### 2. 核心交互界面（增加结构化Prompt）
<img width="1560" height="912" alt="Image" src="https://github.com/user-attachments/assets/2a85aaa3-c4ec-41a1-a2cf-1bdddeac5894" />
<img width="1534" height="912" alt="Image" src="https://github.com/user-attachments/assets/eb29d608-2b70-4298-8079-2f9e1f64a455" />

## 核心优化亮点
### 1. 突破性的长上下文支持(>32k Tokens)
为了满足复杂医疗场景下的信息综合处理需求，系统进行了严苛的压力测试与升级：
- K值激增：将检索增强生成（RAG）的检索条目数从常规的K=3提升至K=50。
- 超长文本吞吐：单次推理可处理约35,000tokens的上下文信息，证明了系统在海量参考资料下的稳定性。
- 分层展示策略：在app_gradio.py中实现了“后台重计算、前台轻展示”的逻辑——后台模型分析全部50条资料以确保结论严谨，前端界面仅向用户展示相关度最高的Top-5来源，兼顾性能指标与用户体验。

### 2. “沙里淘金”式抗干扰微调(SFT)
针对长上下文检索容易引入噪声（无关信息）的问题，构建了专门的强化版数据集 (prepare_sft_data_pro.py)：
- 噪声注入训练：在构造训练数据时，强制在核心资料（Ground Truth）后拼接随机抽取的无关医疗文本。
- 抗噪能力构建：训练模型在面对“真假参半”的 50 条资料时，能够忽略干扰项，精准锁定核心证据。
- 拒识机制：注入了 200+ 条拒识样本（如“如何写代码”），教会模型在问题超出医疗知识库范围时明确拒绝，防止幻觉产生。

### 3. 严格的“五段式”结构化输出
摒弃了发散式的回答风格，通过 LoRA 微调强制模型遵循行业标准的回复逻辑：
- 背景分析：症状成因简述。
- 缓解方案：实操性强的数字列表。
- 药物指导：非处方药建议（带遵医嘱提醒）。
- 警示说明：就医红线提醒。
- 专业后缀：固定的免责声明。

### 4. 全链路评估体系
- 基线建立：使用 evaluate.py 在K=50的高压环境下完成了Base模型的ROUGE评分测试（当前 ROUGE-L$\approx$0.07），量化了长文本对未微调模型的干扰影响，为后续展示微调带来的性能飞跃确立了基准。

### 5. 技术栈升级与兼容性修复
微调框架：引入 ModelScope 的 SWIFT 框架进行高效 LoRA 训练。
- Gradio 适配：彻底解决了 Gradio 4.x/5.x 版本中 
- Chatbot 组件的数据格式兼容问题（Type Error修复），实现了旧版元组格式向 OpenAI 字典格式的平滑迁移。